# RAG Agent Chatbot (n8n Workflow)

This repository contains an n8n workflow that implements a full **Retrieval-Augmented Generation (RAG) chatbot**.  
The system works in two phases:

1. **Data Ingestion Phase** â€“ Load and embed documents, then store them in Pinecone for semantic retrieval.
2. **Chatbot Phase** â€“ Retrieve relevant chunks from Pinecone and generate intelligent answers using an LLM.

This workflow is useful for:
- Resume-based Q&A chatbots  
- Document-based assistants  
- PDF / Drive knowledge chatbots  
- Context-aware HR or customer support bots  

---

## ğŸ“Œ Workflow Screenshots

### ğŸ”¹ 1. Workflow Overview  
![Workflow Overview](./RAG-base-chatbot.jpg)

---

### ğŸ”¹ 2. Chat Example â€“ Q&A About Projects  
![Chat Example 1](./chatbot.jpg)

---

## ğŸ§  Workflow Architecture

### ğŸ”· Phase 1 â€” Data Ingestion & Vector Store Creation  
This phase triggers when a file is uploaded to Google Drive.

**Nodes Included:**
- Google Drive Trigger  
- Download File  
- Text Loader / PDF Loader  
- Embeddings (Google Gemini / OpenAI)  
- Pinecone Vector Store (Upsert)  

**Purpose:**  
To convert document text into embeddings and store them in Pinecone so that the chatbot can retrieve relevant information later.

---

### ğŸ”· Phase 2 â€” RAG Chatbot Agent  
This phase handles user queries.

**Nodes Included:**
- Chat Trigger (`When chat message received`)  
- Embeddings for query  
- Pinecone Vector Search  
- LLM Agent (Gemini / OpenAI)  
- Simplex Memory (Conversation memory)  

**Purpose:**  
To retrieve context from Pinecone and generate accurate, context-aware responses.

---

## ğŸš€ How It Works

### 1ï¸âƒ£ File Upload â†’ Vector Store  
- User uploads a PDF or document.  
- Workflow downloads and processes the file.  
- The document is split into chunks.  
- Embeddings are generated for each chunk.  
- Chunks + embeddings are stored in Pinecone.

### 2ï¸âƒ£ Ask a Question â†’ Intelligent Response  
- The user types a question into the chat UI.  
- The question is embedded.  
- Pinecone returns top relevant chunks.  
- LLM generates a final answer using retrieved context.

---

## ğŸ§© Requirements

### Accounts / Services Needed
- **n8n**
- **Google Drive API**
- **Pinecone Vector DB**
- **Embedding Provider:** Google Gemini / OpenAI
- **LLM Provider:** Gemini / OpenAI

### n8n Credentials
- Google Drive OAuth  
- Pinecone API Key  
- Gemini / OpenAI API Key  

---

## ğŸ“ Repository Contents

